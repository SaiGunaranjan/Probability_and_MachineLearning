# -*- coding: utf-8 -*-
"""
Created on Thu Jul  6 15:09:30 2023

@author: Sai Gunaranjan
"""

"""
Logistic algorithm

In this script, I have implemented the Logistic regression algorithm. Logistic regression is a
supervised learning algorithm for binary classification when the data is linearly separable to some extent.
It doesnt require a strict constraint on the linear separability. Even though it is called logistic "regression",
it is not a regression algorithm. It is a classification algorithm. But it is called regression because,
in some sense we are regressing the parameter vector W.
The implementation is based on the video lectures of Arun Rajkumar.


For generating the 2 class data/labels, I have used the "make_classification" function of the datasets class imported from
the sklearn package. Here we can mention the number of data samples, dimensionality of the feature vector, number of classes,
number of clusters per class, amount of separation across the classes and so on. This is a very useful function for data generation.


The perceptron algorithm is a deterministic linear classifier i.e P(Y=1/X) = 1 if wTx >= 0 and 0 otherwise.
Also, it works well if the data is linearly separable with a gamma margin. However, the logistic regression algorithm
gives a probabilistic measure for each data point and hence the data need not be strictly linearly separable.
The dot product of the parameter vector W with the data point is used a weight for that data point. So,
the more positve the dot product wTx, greater the probability that the point belongs to class 1. Similarly,
the more negative wTx, lesser the probability that the point belongs to label 1 (or equivalently, greater the probability
the data point belongs to class 0). If the dot product wTx = 0, then it is equally likely to come from class 1,0.
So, now we need to convert these scores for each of the data points to a probabilistic measure.
The probabisitic measure/function mapping (from wTx to probability) should be as follows:
    1. P(Y=1/X) --> 1 as wTx --> infinity
    2. P(Y=1/X) --> 0 as wTx --> -infinity ( or P(Y=0/X) --> 1)
    3. P(Y=1/X) = 0.5 if wTx = 0
One of the function which satisfies this mapping from wTx (-infinity, infinity) to [0,1] is the
logistic/sigmoid function. It is given by 1 / (1 + e**(-wTx)).
With this picture in mind, we can think of each data point being generated by a coin toss whose probability of
head i.e P(Y=1/X) is 1 / (1 + e**(-wTx)). So, more positive the inner product wTx, more likely it is to come from class 1
(It still has a finite non-zero probability of coming from class 0). Similarly, the more negative wTx, the more likely
the data point belongs to class 0 (here also it has a finite non-zero probability of belonging to class 1).
Thus the logistic regression algorithm accomodates non-linearly separable datasets as well.
The update rule for the logistic regression algo is based on the maximizing the log-likelihood function.
But due to the complex form of the sigmoid function, we do not get a close form expression for the maximizer W.
Instead we find the gradient and update the W at each iteration. The derivation is clearly explained in Arun's video lectures.
The update equation for W is a linear combination of the data points each weighted by the difference of the true label
and the estimated label (as given by the sigmoid function value for that data point). The update equation is as follows:
    W(k+1) = W(k) + alpha * summation (Xi (Yi-P(Y=1/Xi))), where P(Y=1/Xi) is the sigmoid function.
From this equation, we can clearly see that the data points for which the true labels and the estimated labels (rom sigmoid function)
match, do not contribute to the W update. It is those points for which there is a mismatch between the true labels
and estimated labels, that contribute to the W update. This is key observation which lays the foundation for
more sophisticated algos like SVms, etc.

Note:
    In perceptron algorithm, we treat the labels as +1, -1.
    In logistic regression algorithm, we treat the labels as 1, 0
The above assumptions just make the math easier.

"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets
from scipy.special import expit as sigmoid
from scipy import special

plt.close('all')

""" Create the dataset for binary class"""
Data, labels = datasets.make_classification(n_samples=1000,n_features=2,n_classes=2,n_clusters_per_class=1,n_redundant=0,\
                                            class_sep=1,random_state=1) # random_state = 2, class_sep =1 causing issues
numDataPoints = Data.shape[0]
numFeatures = Data.shape[1]
numTrainingData = int(np.round(0.7 * numDataPoints))


trainingData = Data[0:numTrainingData,:]
trainingLabels = labels[0:numTrainingData]

trainingDataClass0 = trainingData[trainingLabels==0,:]
trainingDataClass1 = trainingData[trainingLabels==1,:]

testingData = Data[numTrainingData::,:]
testingLabels = labels[numTrainingData::]

testingDataClass0 = testingData[testingLabels==0,:]
testingDataClass1 = testingData[testingLabels==1,:]

""" Perceptron training phase """

""" Cap the maximum number of iterations of the Logistic regression algorithm."""
numMaxIterations = 100

""" The dataset might have a bias and need not always be around the origin. It could be shifted.
For example, the data might be separated by the line wTx = -5. We do not know the bias apriori. So,
to handle this, we do the following.
We include the unknown bias also as another parameter to the w vector. We then also append a 1 to the feature vector
([x, 1]). Hence, the dimensionality of both the feature vector and the parameter vector w are increased by 1.
"""
wVec = np.zeros((numFeatures+1,),dtype=np.float32) # +1 to take care of the bias in the data

trainingDataExt = np.hstack((trainingData,np.ones((numTrainingData,1)))) # Appending 1s to the training data.
logLikelihood = np.zeros((numMaxIterations,),dtype=np.float32)
for ele1 in range(numMaxIterations):
    alpha = 1/(ele1+1) # Learning rate/step size. Typically set as 1/t or 1/t**2
    wTx = trainingDataExt @ wVec

    # p_Yequal1_condX = 1 / (1 + np.exp(-wTx))
    # logLikelihood[ele1] = np.sum(-wTx + wTx*trainingLabels - np.log(1+np.exp(-wTx)))
    """ The above piece of code sufferes from overflows. This is because, when the dot product wTx becomes very large
    (both positive and negative),
    it can cause over flow while evaluating exp(-wTx) and also log(1+exp(-wTx)). This will result in NaNs.
    To avoid this, we use the inbuilt sigmoid function and the special.logsumexp function which evaluates
    log(sum of powers of e). We can pass the powers of e as an input vector to this function. The overflows are
    gracefully handled by these functions"""
    p_Yequal1_condX = sigmoid(wTx)
    temp1 = np.zeros((numTrainingData))
    temp2 = np.hstack((temp1[:,None],-wTx[:,None]))
    logLikelihood[ele1] = np.sum(-wTx + wTx*trainingLabels - special.logsumexp(temp2,axis=1)) #np.sum(-wTx + wTx*trainingLabels - np.log(sigmoid(wTx)))
    wVec = wVec + alpha*np.sum(trainingDataExt * (trainingLabels[:,None] - p_Yequal1_condX[:,None]),axis=0)


""" Testing phase"""
numTestingData = testingData.shape[0]
testingDataExt = np.hstack((testingData,np.ones((numTestingData,1)))) # Appending 1s to the test data as well.

wtx_test = testingDataExt @ wVec
estLabels = np.zeros((testingLabels.shape),dtype=np.int32)
estLabels[wtx_test>=0] = 1
estLabels[wtx_test<0] = 0

accuracy = np.mean(estLabels == testingLabels) * 100
print('\nAccuracy of clasification = {0:.2f} % \n'.format(accuracy))

""" Plotting the separating hyper plane"""
separatingLineXcordinate = np.linspace(np.amin(Data[:,0])-2,np.amax(Data[:,0])+2,100)
separatingLineYcordinate = (-wVec[-1] - wVec[0]*separatingLineXcordinate)/wVec[1]

plt.figure(1,figsize=(20,10),dpi=200)
plt.subplot(1,2,1)
plt.title('Training data')
plt.scatter(trainingDataClass0[:,0],trainingDataClass0[:,1],color='red')
plt.scatter(trainingDataClass1[:,0],trainingDataClass1[:,1],color='green')
plt.plot(separatingLineXcordinate,separatingLineYcordinate)
plt.grid(True)
plt.xlabel('x1')
plt.ylabel('y1')
plt.ylim([np.amin(Data[:,1])-2,np.amax(Data[:,1])+2])
plt.xlim([np.amin(Data[:,0])-2,np.amax(Data[:,0])+2])
# plt.axis('scaled')

plt.subplot(1,2,2)
plt.title('Testing data')
plt.scatter(testingDataClass0[:,0],testingDataClass0[:,1],color='red')
plt.scatter(testingDataClass1[:,0],testingDataClass1[:,1],color='green')
plt.plot(separatingLineXcordinate,separatingLineYcordinate)
plt.grid(True)
plt.xlabel('x1')
plt.ylabel('y1')
plt.ylim([np.amin(Data[:,1])-2,np.amax(Data[:,1])+2])
plt.xlim([np.amin(Data[:,0])-2,np.amax(Data[:,0])+2])


plt.figure(2,figsize=(20,10),dpi=200)
plt.title('Log likelihood vs iterations')
plt.plot(np.arange(1,numMaxIterations),logLikelihood[1::],'-o')
plt.xlabel('Iterations')
plt.grid(True)
